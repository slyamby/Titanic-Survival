{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc3e82d-8112-4832-ab64-06d8e0ab5806",
   "metadata": {},
   "source": [
    "### Titanic - Machine Learning from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e9384-61b9-41f9-a688-0ed22daf27d7",
   "metadata": {},
   "source": [
    "### Extract Data from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23bad3e4-29b5-4bf8-9575-849c59cd180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/anaconda3/envs/tf_env/lib/python3.10/site-packages (from imbalanced-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45049033-6f1c-46c9-a92d-289c73b124dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "raw_csv_data = pd.read_csv('train.csv')\n",
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7611ad2c-adb3-4a91-9562-eddcd71d8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "print(raw_csv_data[\"Sex\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a396cd-0df3-40fd-aee9-46735d49f365",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = pd.get_dummies(raw_csv_data[[\"Pclass\",\"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]], drop_first=True) # drop first avoids a dummy trap\n",
    "numerical = raw_csv_data[['Age','Fare']].fillna(raw_csv_data[['Age','Fare']].mean()) # Fill NaNs smartly\n",
    "inputs = pd.concat([numerical,categorical], axis=1)\n",
    "\n",
    "targets = raw_csv_data[\"Survived\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed95c16c-2634-403b-bc74-d4ac1b0af7bd",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20969678-e2b8-440b-8423-911147410ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Fill NaN values using mean for numerical data\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "inputs_imputed = imputer.fit_transform(inputs)  # This returns a NumPy array\n",
    "\n",
    "smote = SMOTE()\n",
    "\n",
    "unscaled_inputs_equal_priors, target_equal_priors = smote.fit_resample(inputs_imputed, targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3992927-492d-417b-866e-e40a19772749",
   "metadata": {},
   "source": [
    "### Standardize the Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca31ddd-85e0-46f0-82f8-a9fe21db8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99a6889-82e5-469d-b709-aa785c944986",
   "metadata": {},
   "source": [
    "### Shuffle the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5251f9e5-08f2-4376-919b-e8ec7ef63172",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = target_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75431bdd-cc4f-4c7a-85c7-23b312cf58e0",
   "metadata": {},
   "source": [
    "### Split the dataset into train and validate and initialize the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7f243f8-38b3-45d9-bb77-3b6f83d38011",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = samples_count - train_samples_count\n",
    "\n",
    "# Splitting data\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:]\n",
    "validation_targets = shuffled_targets[train_samples_count:]\n",
    "\n",
    "train_columns = inputs.columns  # 'inputs' is from training preprocessing\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_categorical = pd.get_dummies(test_data[[\"Pclass\", \"Name\", \"Sex\", \"SibSp\", \"Parch\", \"Ticket\", \"Embarked\"]], drop_first=True)\n",
    "test_numerical = test_data[['Age', 'Fare']]\n",
    "test_inputs = pd.concat([test_numerical, test_categorical], axis=1)\n",
    "\n",
    "# Reindex to match the training columns exactly, filling missing columns with 0\n",
    "test_inputs = test_inputs.reindex(columns=train_columns, fill_value=0)\n",
    "\n",
    "# Scale using the same scaler as training (optional if using `preprocessing.scale`)\n",
    "test_inputs = preprocessing.scale(test_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42f626c5-a594-4261-9460-15bf8136508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def validate_and_clean_data(inputs, targets=None):\n",
    "    \"\"\"\n",
    "    Validates and cleans input and target arrays:\n",
    "    - Replaces NaNs and Infs with safe defaults\n",
    "    - Ensures targets are integers (0 or 1)\n",
    "    \n",
    "    Parameters:\n",
    "        inputs (np.ndarray): Feature matrix.\n",
    "        targets (np.ndarray, optional): Target labels. Can be None for test sets.\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_inputs (np.ndarray), cleaned_targets (np.ndarray or None)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean inputs\n",
    "    cleaned_inputs = np.nan_to_num(inputs, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "    \n",
    "    if targets is not None:\n",
    "        # Check for NaNs/Infs in targets\n",
    "        cleaned_targets = np.nan_to_num(targets, nan=0.0, posinf=1, neginf=0)\n",
    "        \n",
    "        # Force integer 0 or 1\n",
    "        cleaned_targets = (cleaned_targets > 0.5).astype(int)\n",
    "        \n",
    "        # Sanity check\n",
    "        unique = np.unique(cleaned_targets)\n",
    "        if not np.array_equal(unique, [0, 1]) and not np.array_equal(unique, [0]) and not np.array_equal(unique, [1]):\n",
    "            raise ValueError(f\"Targets contain invalid values: {unique}\")\n",
    "        \n",
    "        return cleaned_inputs, cleaned_targets\n",
    "    \n",
    "    return cleaned_inputs, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53be99bf-574e-4f0d-bee5-e53c9630994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data before saving\n",
    "train_inputs, train_targets = validate_and_clean_data(train_inputs, train_targets)\n",
    "validation_inputs, validation_targets = validate_and_clean_data(validation_inputs, validation_targets)\n",
    "test_inputs, _ = validate_and_clean_data(test_inputs)\n",
    "\n",
    "# Save cleaned data\n",
    "np.savez('titanic_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('titanic_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('titanic_data_test', inputs=test_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84859632-0f4a-4aef-9a61-302db2704b08",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92609d19-6c0d-42f2-b23e-f032dee9c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a temporary variable npz, where we will store each of the three Audiobooks datasets\n",
    "npz = np.load('titanic_data_train.npz',allow_pickle=True)\n",
    "\n",
    "# we extract the inputs using the keyword under which we saved them\n",
    "# to ensure that they are all floats, let's also take care of that\n",
    "train_inputs = npz['inputs'].astype(float)\n",
    "# targets must be int because of sparse_categorical_crossentropy (we want to be able to smoothly one-hot encode them)\n",
    "train_targets = npz['targets'].astype(int)\n",
    "\n",
    "# we load the validation data in the temporary variable\n",
    "npz = np.load('titanic_data_validation.npz',allow_pickle=True)\n",
    "# we can load the inputs and the targets in the same line\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(float), npz['targets'].astype(int)\n",
    "\n",
    "# we load the test data in the temporary variable\n",
    "npz = np.load('titanic_data_test.npz',allow_pickle=True)\n",
    "\n",
    "test_inputs = npz['inputs'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adfe6af7-cf80-4fd9-a086-cd94528eddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train inputs shape: (878, 8)\n",
      "Train targets shape: (878,)\n",
      "Validation inputs shape: (220, 8)\n",
      "Validation targets shape: (220,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train inputs shape:\", train_inputs.shape)\n",
    "print(\"Train targets shape:\", train_targets.shape)\n",
    "print(\"Validation inputs shape:\", validation_inputs.shape)\n",
    "print(\"Validation targets shape:\", validation_targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286d8e0f-4f70-4301-8411-05a30dbc00e0",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b130f7f4-7f58-4983-a1c9-2c34fe2e14b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 - 1s - 23ms/step - accuracy: 0.7084 - loss: 0.6021 - val_accuracy: 0.7727 - val_loss: 0.5241\n",
      "Epoch 2/100\n",
      "28/28 - 0s - 2ms/step - accuracy: 0.7904 - loss: 0.4888 - val_accuracy: 0.7773 - val_loss: 0.4782\n",
      "Epoch 3/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.7916 - loss: 0.4567 - val_accuracy: 0.7955 - val_loss: 0.4713\n",
      "Epoch 4/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.7984 - loss: 0.4352 - val_accuracy: 0.7909 - val_loss: 0.4636\n",
      "Epoch 5/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8052 - loss: 0.4321 - val_accuracy: 0.7955 - val_loss: 0.4584\n",
      "Epoch 6/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8166 - loss: 0.4215 - val_accuracy: 0.7909 - val_loss: 0.4601\n",
      "Epoch 7/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8144 - loss: 0.4131 - val_accuracy: 0.7955 - val_loss: 0.4552\n",
      "Epoch 8/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8200 - loss: 0.4183 - val_accuracy: 0.7955 - val_loss: 0.4552\n",
      "Epoch 9/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8121 - loss: 0.4232 - val_accuracy: 0.8000 - val_loss: 0.4520\n",
      "Epoch 10/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8121 - loss: 0.4221 - val_accuracy: 0.8045 - val_loss: 0.4513\n",
      "Epoch 11/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8018 - loss: 0.4261 - val_accuracy: 0.7909 - val_loss: 0.4487\n",
      "Epoch 12/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8189 - loss: 0.4067 - val_accuracy: 0.7864 - val_loss: 0.4622\n",
      "Epoch 13/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8132 - loss: 0.4057 - val_accuracy: 0.7864 - val_loss: 0.4581\n",
      "Epoch 14/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8109 - loss: 0.4018 - val_accuracy: 0.7818 - val_loss: 0.4546\n",
      "Epoch 15/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8303 - loss: 0.3947 - val_accuracy: 0.7909 - val_loss: 0.4513\n",
      "Epoch 16/100\n",
      "28/28 - 0s - 1ms/step - accuracy: 0.8257 - loss: 0.4009 - val_accuracy: 0.8045 - val_loss: 0.4530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14fa9cc40>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input and output sizes\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model definition\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(train_inputs.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_inputs, train_targets,\n",
    "          epochs=100,\n",
    "          validation_data=(validation_inputs, validation_targets),\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59becbf8-b9e0-4913-8191-19c2c6afa9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities for the test data\n",
    "test_prediction_probabilities = model.predict(test_inputs)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "test_predictions = (test_prediction_probabilities >= 0.5).astype(int)\n",
    "\n",
    "#View the first few predictions\n",
    "print(test_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6cf058a4-d4f7-4d42-aa53-b11f0a7a5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_data['PassengerId'],\n",
    "    'Survived': test_predictions.ravel()  # flatten to 1D array\n",
    "})\n",
    "submission.to_csv('titanic_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
